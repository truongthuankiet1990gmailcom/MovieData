{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Introduction](#intro)\n",
    "2. [Libraries](#libraries)\n",
    "3. [Strategies](#strategies)\n",
    "4. [Evaluation](#evaluation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id = \"intro\"> 1. Introduction </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is created to test strategies to impute missing values of numerical columns and to evaluate which one is most appropriate and comprehensive to apply for preprocessing stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id = \"libraries\"> 2. Libraries </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "from scipy.stats import sem\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('../Data/retyped_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id = \"strategies\"> <h1>3. Strategies </h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we concluded from the data exploration section, missing data of all numerical columns is **MAR**, therefore, we need to have an appropriate approach to fill. \n",
    "- So we decided to test **Similarity based strategy**, **KNN Imputer** and **Decision Tree Regressor** and check metrics to choose which one is the most suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list to store metrics of each model\n",
    "similarity_model = []\n",
    "knn_model = []\n",
    "decision_tree_model = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id = \"similarity\"> <h2> Similarity based strategy </h2> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Similarity-Based Imputation:** Missing values in the ratings matrix are filled by leveraging similarity scores, which are calculated based on the inverse of the mean absolute difference between users and critics' ratings. This ensures that missing values are imputed based on the preferences of users with similar rating patterns.\n",
    "- **Handling Missing Values:** The use of np.nanmean and logical operations effectively handles NaN values during computation, ensuring accurate similarity and weight calculations. Furthermore, adding a small epsilon (0.001) prevents division by zero errors, enhancing the robustness of the similarity computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 32\n",
    "start = 0\n",
    "end = 32\n",
    "\n",
    "# Define the function to calculate similarities\n",
    "def calculate_similarities(ratings, batch_start, batch_end):\n",
    "    # Select the batch of users\n",
    "    batch_ratings = ratings[batch_start:batch_end]\n",
    "    \n",
    "    # Calculate the absolute difference between the batch and all users\n",
    "    abs_diff = np.abs(ratings - batch_ratings.reshape(batch_end - batch_start, 1, ratings.shape[1]))\n",
    "    \n",
    "    # Calculate the mean absolute difference across movies, ignoring NaN values\n",
    "    mean_diff = np.nanmean(abs_diff, axis=2)\n",
    "    \n",
    "    # Compute similarity as the inverse of the mean absolute difference\n",
    "    similarities = 1 / (mean_diff + 0.001)  # Adding a small epsilon to avoid division by zero\n",
    "    similarities[np.isnan(similarities)] = 0\n",
    "    return similarities\n",
    "\n",
    "def fill_missing(data, batch_size = 32):\n",
    "    n_movies = data.shape[0]\n",
    "    filled_ratings = np.empty_like(data)\n",
    "    num_batches = int(np.ceil(n_movies / batch_size))\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = min((i + 1) * batch_size, n_movies)\n",
    "\n",
    "        similarities = calculate_similarities(data, start, end)\n",
    "        \n",
    "        weights = ~np.isnan(data) * similarities.reshape(end - start, -1, 1)\n",
    "        weights /= weights.sum(axis=1, keepdims=True)\n",
    "\n",
    "        filled_ratings[start:end] = np.nansum(data * weights, axis=1)\n",
    "\n",
    "    return filled_ratings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Evaluate each columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_21984\\490181334.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  tmp_data_2[col].fillna(filled_df[col], inplace=True)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_21984\\490181334.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  tmp_data_2[col].fillna(filled_df[col], inplace=True)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_21984\\490181334.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  tmp_data_2[col].fillna(filled_df[col], inplace=True)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_21984\\490181334.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  tmp_data_2[col].fillna(filled_df[col], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "columns = ['Tomatoes CriticScore', 'Tomatoes UserScore', 'Metascore', 'Meta UserScore']\n",
    "\n",
    "test_size = 0.3\n",
    "\n",
    "#Evalue each column seperatly\n",
    "for test_col in columns:\n",
    "    #Get a copy of data but remove all null value for testing\n",
    "    raw_data_copy = raw_data.copy()\n",
    "    raw_data_copy.dropna(inplace=True)\n",
    "    raw_data_copy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    #Get sample with size of 30%\n",
    "    test_rows = raw_data_copy.sample(frac=test_size, random_state=42).index\n",
    "\n",
    "    #Get y true value from the dataset\n",
    "    y_test = raw_data_copy.loc[test_rows, test_col].copy()\n",
    "\n",
    "    #Assign it's as nan value for imputing\n",
    "    raw_data_copy.loc[test_rows, test_col] = np.nan\n",
    "\n",
    "    #Perform imputing missing value using similarity\n",
    "    tmp_data = raw_data_copy.copy()\n",
    "    tmp_data['id'] = tmp_data.index\n",
    "    tmp_data['Meta UserScore'] = tmp_data['Meta UserScore'] * 10\n",
    "\n",
    "    tmp_data = tmp_data[['id','Tomatoes CriticScore', 'Tomatoes UserScore', 'Metascore', 'Meta UserScore']].to_numpy()\n",
    "\n",
    "    filled_ratings = fill_missing(tmp_data)\n",
    "    filled_nanvals = filled_ratings[np.isnan(tmp_data)]\n",
    "\n",
    "    tmp_data[np.isnan(tmp_data)] = filled_nanvals\n",
    "\n",
    "    filled_df = pd.DataFrame(\n",
    "        filled_ratings[:, 1:],\n",
    "        columns=['Tomatoes CriticScore', 'Tomatoes UserScore', 'Metascore', 'Meta UserScore']\n",
    "    )\n",
    "\n",
    "    filled_df['Meta UserScore'] /= 10\n",
    "    tmp_data_2 = raw_data_copy.copy()\n",
    "\n",
    "    for col in filled_df.columns:\n",
    "        tmp_data_2[col].fillna(filled_df[col], inplace=True)\n",
    "\n",
    "    #Get y predicted (column after imputing)\n",
    "    y_pred = tmp_data_2.loc[test_rows, test_col]\n",
    "\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Store the metrics of each model\n",
    "    similarity_model.append({\n",
    "        'Column': test_col,\n",
    "        'MAE': mae,\n",
    "        'MSE': mse,\n",
    "        'R2': r2\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id = \"knn\"> <h2> KNN imputer strategy </h2> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using 20 neighbors with uniform weighting to predict missing values.\n",
    "- The imputation process fills the NaN values in the masked test column based on the nearest neighbors' values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare the data (numeric columns only)\n",
    "columns = ['Tomatoes CriticScore', 'Tomatoes UserScore', 'Metascore', 'Meta UserScore']\n",
    "test_size = 0.3\n",
    "\n",
    "for test_col in columns:\n",
    "    # Step 1: Prepare the test dataset\n",
    "    raw_data_copy = raw_data.copy()\n",
    "    raw_data_copy.dropna(inplace=True)\n",
    "    raw_data_copy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Select test rows (30% of the data)\n",
    "    test_rows = raw_data_copy.sample(frac=test_size, random_state=42).index\n",
    "    y_test = raw_data_copy.loc[test_rows, test_col].copy()\n",
    "\n",
    "    # Mask test column values (set them to NaN for imputation)\n",
    "    raw_data_copy.loc[test_rows, test_col] = np.nan\n",
    "\n",
    "    # Step 2: Apply KNN Imputation\n",
    "    knn_imputer = KNNImputer(n_neighbors=20, weights=\"uniform\")\n",
    "    imputed_data = knn_imputer.fit_transform(raw_data_copy[columns])\n",
    "\n",
    "    # Reconstruct the imputed DataFrame\n",
    "    imputed_df = pd.DataFrame(imputed_data, columns=columns)\n",
    "\n",
    "    # Step 3: Evaluate the imputed values\n",
    "    y_pred = imputed_df.loc[test_rows, test_col]\n",
    "\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Store the metrics of each model\n",
    "    knn_model.append({\n",
    "        'Column': test_col,\n",
    "        'MAE': mae,\n",
    "        'MSE': mse,\n",
    "        'R2': r2\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id = \"decision\"> <h2> Decision Tree Regressor strategy </h2> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observes features in this case we will use all scores columns and trains a model in the structure of a tree to predict scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare the data (numeric columns only)\n",
    "columns = ['Tomatoes CriticScore', 'Tomatoes UserScore', 'Metascore', 'Meta UserScore']\n",
    "test_size = 0.3\n",
    "\n",
    "for test_col in columns:\n",
    "    # Step 1: Prepare the test dataset\n",
    "    raw_data_copy = raw_data.copy()\n",
    "    raw_data_copy.dropna(inplace=True)\n",
    "    raw_data_copy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    #Train Test split\n",
    "    train_col = columns.copy()\n",
    "    train_col.remove(test_col)\n",
    "\n",
    "    X = raw_data_copy[train_col].values\n",
    "    y = raw_data_copy[test_col].values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "    #Train model\n",
    "    decision_tree = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "    decision_tree.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = decision_tree.predict(X_test)\n",
    "        \n",
    "    #Evaluate\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Store the metrics of each model\n",
    "    decision_tree_model.append({\n",
    "        'Column': test_col,\n",
    "        'MAE': mae,\n",
    "        'MSE': mse,\n",
    "        'R2': r2\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id = \"evaluation\"> <h1>4. Evaluation </h1> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 MAE     MSE    R2\n",
      "                mean    mean  mean\n",
      "Model                             \n",
      "Decision Tree   6.12   88.12  0.70\n",
      "KNN             6.23   88.30  0.71\n",
      "Similarity     11.83  262.07  0.32\n"
     ]
    }
   ],
   "source": [
    "# Convert the metrics of each model to DataFrames\n",
    "similarity_model = pd.DataFrame(similarity_model)\n",
    "knn_model = pd.DataFrame(knn_model)\n",
    "decision_tree_model = pd.DataFrame(decision_tree_model)\n",
    "\n",
    "# Make sure that similarity_model, knn_model, and decision_tree_model have columns 'MAE', 'MSE', 'R2' and 'Model'\n",
    "similarity_model['Model'] = 'Similarity'\n",
    "knn_model['Model'] = 'KNN'\n",
    "decision_tree_model['Model'] = 'Decision Tree'\n",
    "\n",
    "# Combine the metrics of all models\n",
    "all_models = pd.concat([similarity_model, knn_model, decision_tree_model], ignore_index=True)\n",
    "\n",
    "# Create a table to summarize the metrics of all models\n",
    "summary_table = all_models.groupby(['Model']).agg({\n",
    "    'MAE': ['mean'],\n",
    "    'MSE': ['mean'],\n",
    "    'R2': ['mean']\n",
    "}).round(2)\n",
    "\n",
    "print(summary_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KNN and Decision Tree are performing similarly in terms of MAE, MSE, and R². Both models are significantly better than the Similarity model.\n",
    "- However, KNN would likely be the better choice because:\n",
    "    - Movie ratings are typically continuous, with values influenced by the preferences of users/critics (neighbors). KNN works well for this kind of problem because it relies on the similarity between users/critics, which is particularly useful in collaborative filtering tasks like movie rating prediction.\n",
    "    - KNN can predict the missing ratings by considering the ratings of similar users or movies, which is a natural fit for this type of task.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
